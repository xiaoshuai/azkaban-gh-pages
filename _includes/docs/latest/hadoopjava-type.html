<h2 id="hadoopjava-type">hadoopJava Type</h2>

<p>In large part, this is the same <code>java</code> type. The difference is its ability to talk to a Hadoop cluster securely, via Hadoop tokens. Most Hadoop job types can be created by running a hadoopJava job, such as Pig, Hive, etc.</p>

<h3>How To Use</h3>

<p>The <code>hadoopJava</code> type runs user java program after all. Upon execution, it tries to construct an object that has the constructor signature of <code>constructor(String, Props)</code> and runs its <code>run</code> method. If user wants to cancel the job, it tries the user defined <code>cancel</code> method before doing a hard kill on that process.</p>

<p>The <code>hadoopJava</code> job type talks to a secure cluster via Hadoop tokens. The admin should specify <code>obtain.binary.token=true</code> if the Hadoop cluster security is turned on. Before executing a job, Azkaban will obtain name node token and job tracker tokens for this job. These tokens will be written to a token file, to be picked up by user job process during its execution. After the job finishes, Azkaban takes care of canceling these tokens from name node and job tracker. </p>

<p>Since Azkaban only obtains the tokens at the beginning of the job run, and does not requesting new tokens or renew old tokens during the execution, it is important that the job does not run longer than configured token life.</p>

<p>If there are multiple job submissions inside the user program, the user should also take care not to have a single MR step cancel the tokens upon completion, thereby failing all other MR steps when they try to authenticate with Hadoop services.</p>

<p>In many cases, it is also necessary to add the following code to make sure user program picks up the Hadoop tokens in "conf" or "jobconf" like the following:</p>

<pre class="code">
// Suppose this is how one gets the conf
Configuration conf = new Configuration();

if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
    conf.set("mapreduce.job.credentials.binary", System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
}
</pre>

<p>Here are some common configurations that make a <code>hadoopJava</code> job for a user:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hadoopJava</code></td>
    </tr>
    <tr>
      <td>job.class</td>
      <td>The fully qualified name of the user job class.</td>
    </tr>
    <tr>
      <td>classpath</td>
      <td>The resources that should be on the execution classpath, accessible to the local filesystem.</td>
    </tr>
    <tr>
      <td>main.args</td>
      <td>Main arguments passed to user program.</td>
    </tr>
    <tr>
      <td>dependencies</td>
      <td>The other jobs in the flow this job is dependent upon.</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The Hadoop user this job should run under.</td>
    </tr>
    <tr>
      <td>method.run</td>
      <td>The run method, defaults to <em>run()</em></td>
    </tr>
    <tr>
      <td>method.cancel</td>
      <td>The cancel method, defaults to <em>cancel()</em></td>
    </tr>
    <tr>
      <td>getJobGeneratedProperties</td>
      <td>The method user should implement if the output properties should be picked up and passed to the next job.</td>
    </tr>
    <tr>
      <td>jvm.args</td>
      <td>The <code>-D</code> for the new jvm process</td>
    </tr>
    <tr>
      <td>hadoop-inject.FOO</td>
      <td>FOO is automatically added to the Configuration of any Hadoop job launched.</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admin:</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to Hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user Hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and Hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The Hadoop home where the jars and conf resources are installed.</td>
    </tr>
    <tr>
      <td>jobtype.classpath</td>
      <td>The items that every such job should have on its classpath.</td>
    </tr>
    <tr>
      <td>jobtype.class</td>
      <td>Should be set to <code>azkaban.jobtype.HadoopJavaJob</code></td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
  </tbody>
</table>

<p>Since Azkaban job types are named by their directory names, the admin should also make those naming public and consistent.</p>

<div class="bs-callout bs-callout-info">
  <h4>Sample Job Package</h4>
  <p>Here is a sample job package that does a word count. It relies on a Pig job to first upload the text file onto HDFS. One can also manually upload a file and run the word count program alone.The source code is in <code>azkaban-plugins/plugins/jobtype/src/azkaban/jobtype/examples/java/WordCount.java</code></p>
  <a class="btn btn-primary" href="https://s3.amazonaws.com/azkaban2/azkaban2/samplejobs/java-wc.zip" target="_blank"><span class="glyphicon glyphicon-download-alt"></span> Download java-wc.zip</a> (Uploaded May 13, 2013)
</div>
